
################################
## Cluster Configuration File ##
################################

[cluster azhop-slurm]
FormLayout = selectionpanel
Category = Azure HPC OnDemand Platform 

Autoscale = true

    [[node defaults]]
    UsePublicNetwork = false
    Credentials = azure
    ImageName = OpenLogic:CentOS-HPC:7_9-gen2:latest
    SubnetId = {{ cc_subnetid }}
    Region = {{ cc_region }}
    KeyPairLocation = ~/.ssh/cyclecloud.pem
    EnableAcceleratedNetworking = false
    Interruptible = false
    {% if compute_mi_id is defined and compute_mi_id|length %}
    Azure.Identities = {{ compute_mi_id }}
    {% endif %}

    # Slurm autoscaling supports both Terminate and Deallocate shutdown policies
    ShutdownPolicy = Terminate

    CloudInit = '''#!/bin/bash
echo "cloud-init is removing slurm from the image ..." >> /tmp/cloud-init.txt
date >> /tmp/cloud-init.txt
case `grep "^ID=" /etc/os-release | cut -d= -f2 | xargs` in
    centos|rhel|almalinux|rocky|oraclelinux)
        yum remove munge* slurm* -y >> /tmp/cloud-init.txt
        ;;
    ubuntu|debian)
        apt-get remove munge* slurm* -y >> /tmp/cloud-init.txt
        ;;
    *)
        echo "Unsupported OS" >> /tmp/cloud-init.txt
        ;;
esac
echo "cloud-init done" >> /tmp/cloud-init.txt
'''

        [[[configuration]]]
        slurm.install_pkg = azure-slurm-install-pkg-{{cyclecloud_slurm_release}}.tar.gz
        slurm.autoscale_pkg = azure-slurm-pkg-{{cyclecloud_slurm_release}}.tar.gz
        slurm.install = true
        slurm.version = {{ slurm_version }}
        slurm.user.uid = {{ slurm_uid }}
        slurm.user.gid = {{ slurm_gid }}
        munge.user.uid = {{ munge_uid }}
        munge.user.gid = {{ munge_gid }}
        slurm.accounting.enabled = false # because we deploy our scheduler
        #slurm.accounting.url = $configuration_slurm_accounting_url
        #slurm.accounting.user = $configuration_slurm_accounting_user
        #slurm.accounting.password = $configuration_slurm_accounting_password
        #slurm.additional.config = $additional_slurm_config
        #slurm.ha_enabled = $configuration_slurm_ha_enabled
        #slurm.launch_parameters = $configuration_slurm_launch_parameters

        keepalive.timeout = 3600 # The amount of time in seconds to keep a node "alive" if it has not finished installing/configuring software.
        cyclecloud.converge_on_boot = true
        
        # Disable ip-XXXXXXXX hostname generation
        cyclecloud.hosts.standalone_dns.enabled = false
        cyclecloud.hosts.simple_vpc_dns.enabled = false

        # Disable normal NFS exports and mounts
        cyclecloud.mounts.sched.disabled = true
        cyclecloud.mounts.shared.disabled = true
        cyclecloud.exports.sched.disabled = true
        cyclecloud.exports.shared.disabled = true
        cyclecloud.exports.sched.samba.enabled = false
        cyclecloud.exports.shared.samba.enabled = false
        cyclecloud.exports.defaults.samba.enabled = false
        cshared.server.legacy_links_disabled = true

        # Autoscale settings - If not defined default to the CycleCloud value which is 1800 seconds
        {% if autoscale.idle_timeout is defined %}
        cyclecloud.cluster.autoscale.idle_time_after_jobs = {{autoscale.idle_timeout}}
        cyclecloud.cluster.autoscale.idle_time_before_jobs = {{autoscale.idle_timeout}}
        {% endif %}

        [[[volume boot]]] 
        StorageAccountType = StandardSSD_LRS

        [[[cluster-init cyclecloud/slurm:default:{{cyclecloud_slurm_release}}]]]
        [[[cluster-init common:default:1.0.0]]]

        [[[configuration cyclecloud.mounts.nfs_sched]]]
        type = nfs
        mountpoint = /sched
        export_path = {{mounts.home.export}}/slurm/config
        address = {{mounts.home.server}}
        options = {{ mounts.home.options | default("rw,hard,rsize=262144,wsize=262144,vers=3,tcp,_netdev",true) }}

    [[node nodearraybase]]
    Abstract = true
        [[[configuration]]]
        slurm.autoscale = true
        #slurm.node_prefix = ${ifThenElse(NodeNamePrefix=="Cluster Prefix", StrJoin("-", ClusterName, ""), NodeNamePrefix)}
        slurm.use_nodename_as_hostname = true
        slurm.dampen_memory = 8 # Reservation of 8% of the node's memory
        slurm.use_pcpu = false # set pcpu = false for all hyperthreaded VMs - doesn't seems to impact non hyperthreaded VMs
        [[[cluster-init cyclecloud/slurm:execute:{{cyclecloud_slurm_release}}]]]

{% for queue in cc_queues %}
    [[nodearray {{ queue.name }}]]
    Extends = nodearraybase
    MachineType = {{ queue.vm_size }} 
    MaxCoreCount = {{ queue.max_core_count }}
  {% if queue.EnableAcceleratedNetworking is defined %}
    EnableAcceleratedNetworking = {{ queue.EnableAcceleratedNetworking }}
  {% endif %}
  {% if queue.spot is defined %}
    Interruptible = {{queue.spot}}
  {% endif %}
    Azure.MaxScaleSetSize = {{ queue.MaxScaleSetSize | default(100) }}
    # Lookup image version for that queue
  {% if cc_image_lookup is iterable and queue.name in cc_image_lookup %}
    ImageName = {{ cc_image_lookup[queue.name] }}
  {% else %}
    ImageName = {{ queue.image }}
  {% endif %}
  {% if queue.plan is defined %}
    {% set plan_details = queue.plan.split(':') %}
    ImagePlan.Publisher = {{ plan_details[0] }}
    ImagePlan.Product = {{ plan_details[1] }}
    ImagePlan.Name = {{ plan_details[2] }}
  {% endif %}
        [[[configuration]]]
        slurm.partition = {{ queue.name }}
      {% if loop.index == 1 %}
        slurm.default_partition = true
      {% endif %}
      {% if queue.ColocateNodes is defined %}
        slurm.hpc = {{ queue.ColocateNodes }}
      {% else %}
        slurm.hpc = false
      {% endif %}
      {% if queue.idle_timeout is defined %}
        cyclecloud.cluster.autoscale.idle_time_after_jobs = {{queue.idle_timeout}}
        cyclecloud.cluster.autoscale.idle_time_before_jobs = {{queue.idle_timeout}}
      {% endif %}
        [[[cluster-init enroot:default:1.0.0]]]
{% endfor %}
