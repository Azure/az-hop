# This looks crazy but in order for this playbook to run from a pipeline, the jumpbox dummy need to be added, otherwise there is an unable to connect message
- name: jumpbox dummy
  hosts: jumpbox
  become: true

- hosts: scheduler, ondemand, ccportal, grafana
  become: true
  gather_facts: no
  vars_files:
    - '{{global_config_file}}'

  tasks:
  - name: Wait 300 seconds for the nodes to be ready
    wait_for_connection:
      timeout: 300
  - name: Gather facts for first time
    setup:

  - name: Read Password from KV
    command: az keyvault secret show --vault-name {{key_vault}} -n {{admin_user}}-password --query "value" -o tsv
    delegate_to: localhost
    connection: local
    register: password
    become: false
    run_once: true

  - name: Install Telegraf
    include_role:
      name: telegraf
    vars:
      influxdb_username: "{{admin_user}}"
      influxdb_password: "{{password.stdout}}"
      influxdb_database_name: "telegraf"
      telegraf_influxdb_urls: 
        - "http://{{ grafana.name | default('grafana') }}:8086"

- name: OnDemand Metrics
  hosts: ondemand
  become: true
  vars_files:
    - '{{global_config_file}}'

  tasks:
    - name: enable apache mod_status
      copy:
        dest: /opt/rh/httpd24/root/etc/httpd/conf.d/mod_status.conf
        content: |
          <Location "/server-status">
              SetHandler server-status
          </Location>
      when: 
        - ansible_distribution == 'CentOS'
        - ansible_distribution_major_version == '7'

    - name: enable apache mod_status
      copy:
        dest: /etc/httpd/conf.d/mod_status.conf
        content: |
          <Location "/server-status">
              SetHandler server-status
          </Location>
      when: 
        - ansible_distribution == 'AlmaLinux'
        - ansible_distribution_major_version == '8'

    - name: add apache metrics to telegraf config
      blockinfile:
        path: /etc/telegraf/telegraf.conf
        block: |
          [[inputs.apache]]
            urls = ["http://localhost/server-status?auto"]
            insecure_skip_verify = true

    - name: restart ood
      shell: systemctl try-restart httpd24-httpd.service httpd24-htcacheclean.service
      when: 
        - ansible_distribution == 'CentOS'
        - ansible_distribution_major_version == '7'

    - name: Ensure apache is running.
      service: 
        name: httpd
        state: started
        enabled: yes
      when: 
        - ansible_distribution == 'AlmaLinux'
        - ansible_distribution_major_version == '8'

    - name: Ensure apache is restarted
      service: 
        name: apache2
        state: restarted
        enabled: yes
      when: 
        - ansible_distribution == 'Ubuntu'

- name: PBS metrics
  hosts: scheduler
  become: true
  vars_files:
    - '{{global_config_file}}'
  
  tasks:
  - name: create telegraf plugin directory
    file:
      path: /opt/telegraf/scripts
      state: directory
      mode: 0755
      owner: telegraf

  - name: add PBS scripts
    block:
      - name: create collect_pbs_stats script
        copy:
          dest: /opt/telegraf/scripts/collect_pbs_stats.py
          mode: 0755
          owner: telegraf
          content: |
            #!/usr/bin/env python3

            # How to run in telegraf
            #
            # [[inputs.execd]]
            #  command = ["/opt/telegraf/scripts/collect_pbs_stats.py"]
            #  signal = "STDIN"

            import json
            import subprocess
            import sys

            def read_pbsnodes():
                    # output:
                    # nodes,slot_type=<slot_type>,state=<state> <count>
                    try:                    
                            data = {}
                            res = json.loads(subprocess.check_output(["/opt/pbs/bin/pbsnodes", "-a", "-F", "json"], stderr=subprocess.DEVNULL))
                            for node in res.get("nodes", {}).keys():
                                    details = res["nodes"][node]
                                    slot_type = details["resources_available"]["slot_type"]
                                    state = details["state"].replace("-", "_")
                                    key = ":".join([state, slot_type])
                                    if key not in data: data[key] = 0
                                    data[key] += 1
                            
                            for key in data.keys():
                                    state, slot_type = key.split(":")
                                    print("pbsnodes,slot_type="+slot_type+",state="+state+" nodes="+str(data[key])+"i")
                    except: 
                            pass

            def read_pbsqueue():
                    # output:
                    # jobs,slot_type=<slot_type>,state=<job_state>,user=<user> <count>
                    #
                    # ["Job_Owner"]:"paedwar"
                    # ["job_state"]:"R"
                    # ["Resource_List"]["select"]:"1:ncpus=1:slot_type=execute:ungrouped=false"
                    #     require: count,slot_type
                    data = {}
                    res = json.loads(subprocess.check_output(["/opt/pbs/bin/qstat", "-f", "-F", "json"]))
                    for job in res.get("Jobs", {}).keys():
                            details = res["Jobs"][job]
                            owner = details["Job_Owner"].split("@")[0]
                            state = details["job_state"]
                            sel = details["Resource_List"]["select"]
                            count = sel.split(":")[0]
                            m_str = "slot_type="
                            m_beg = sel.find(m_str) + len(m_str)
                            m_end = sel.find(":", m_beg)
                            if m_end != -1:
                                    slot_type = sel[m_beg:m_end]
                            else:
                                    slot_type = sel[m_beg:]

                            key = ":".join([state, slot_type, owner])
                            if key not in data: data[key] = 0
                            data[key] += int(count)
                    
                    for key in data.keys():
                            state, slot_type, owner = key.split(":")
                            print("pbsjobs,slot_type="+slot_type+",state="+state+",owner="+owner+" nodes="+str(data[key])+"i")

            if __name__ == "__main__":
                    for _ in iter(sys.stdin.readline, ''):
                            read_pbsnodes()
                            read_pbsqueue()
      - name: add pbs metrics to telegraf config
        blockinfile:
          path: /etc/telegraf/telegraf.conf
          block: |
            [[inputs.execd]]
              command = ["/opt/telegraf/scripts/collect_pbs_stats.py"]
              signal = "STDIN"
    when: (queue_manager == "openpbs" or queue_manager is not defined)

- name: SLURM metrics
  hosts: scheduler
  become: true
  vars_files:
    - '{{global_config_file}}'
  
  tasks:
  - name: create telegraf plugin directory
    file:
      path: /opt/telegraf/scripts
      state: directory
      mode: 0755
      owner: telegraf

  - name: add SLURM scripts
    block:
      - name: create collect_slurm_stats script
        copy:
          dest: /opt/telegraf/scripts/collect_slurm_stats.py
          mode: 0755
          owner: telegraf
          content: |
            #!/usr/bin/env python3

            import json
            import subprocess
            import sys

            def sinfo():
                for sinfo_line in subprocess.check_output(["/bin/sinfo", "-h", "-o", "%R %D %F %T"]).decode().split('\n'):
                    data = sinfo_line.split(' ')
                    if (data[0] != ''):
                        partition = data[0]
                        numnodes = data[1]
                        nodes_by_state = data[2].split('/')
                        state = data[3]
                        print("slurm_nodes,partition="+partition+",state="+state+" nodes="+numnodes+"i")

            def squeue():
                jobsp = {}
                jobs_running = 0
                jobs_pending = 0
                jobs_configuring = 0
                jobs_other = 0
                for sinfo_line in subprocess.check_output(["/bin/squeue", "-h", "-o", "%a %D %T %P"]).decode().split('\n'):
                    data = sinfo_line.split(' ')
                    if (data[0] != ''):
                        account = data[0]
                        numnodes = data[1]
                        state = data[2]
                        partition = data[3]

                        if( partition not in jobsp ):
                            jobsp[partition] = {}
                            jobsp[partition]['RUNNING'] = 0
                            jobsp[partition]['PENDING'] = 0
                            jobsp[partition]['CONFIGURING'] = 0
                            jobsp[partition]['OTHER'] = 0

                        # print("slurm2jobs,account="+account+",state="+state+" nodes="+numnodes+"i")
                        if state == 'RUNNING':
                            jobs_running += 1
                            jobsp[partition]['RUNNING'] += 1
                        elif state == 'PENDING':
                            jobs_pending += 1
                            jobsp[partition]['PENDING'] += 1
                        elif state == 'CONFIGURING':
                            jobs_configuring += 1
                            jobsp[partition]['CONFIGURING'] += 1
                        else:
                            jobs_other += 1
                            jobsp[partition]['OTHER'] += 1

                print("slurm_jobs,state=RUNNING jobs="+str(jobs_running)+"i")
                print("slurm_jobs,state=PENDING jobs="+str(jobs_pending)+"i")
                print("slurm_jobs,state=CONFIGURING jobs="+str(jobs_configuring)+"i")
                print("slurm_jobs,state=OTHER jobs="+str(jobs_other)+"i")

                for key in jobsp:
                    print("slurm_jobsp,partition="+key+",state=RUNNING jobs="+str(jobsp[key]['RUNNING'])+"i")
                    print("slurm_jobsp,partition="+key+",state=PENDING jobs="+str(jobsp[key]['PENDING'])+"i")
                    print("slurm_jobsp,partition="+key+",state=CONFIGURING jobs="+str(jobsp[key]['CONFIGURING'])+"i")
                    print("slurm_jobsp,partition="+key+",state=OTHER jobs="+str(jobsp[key]['OTHER'])+"i")

            sinfo()
            squeue()
      - name: add slurm metrics to telegraf config
        blockinfile:
          path: /etc/telegraf/telegraf.conf
          block: |
            [[inputs.execd]]
              command = ["/opt/telegraf/scripts/collect_slurm_stats.py"]
              signal = "STDIN"
    when: (queue_manager == "slurm")

  - name: restart telegraf
    service:
      name: telegraf
      state: restarted
